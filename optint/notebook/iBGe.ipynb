{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73936646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpy2 imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from optint.data import synthetic_instance\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "print(\"rpy2 imported successfully!\")\n",
    "\n",
    "bidag = importr('BiDAG')\n",
    "base = importr('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e14926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True DAG adjacency matrix:\n",
      "[4][2][0][3|0][1|3]\n",
      "\n",
      "True B matrix:\n",
      "[[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.         -0.53688887  0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.70873987  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "p = 5\n",
    "seed = 42\n",
    "\n",
    "# Create synthetic problem\n",
    "problem = synthetic_instance(\n",
    "    nnodes=p,\n",
    "    DAG_type=\"random\",\n",
    "    sigma_square=0.4 * np.ones(p),\n",
    "    a_size=p,\n",
    "    a_target_nodes=None,\n",
    "    std=False,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "print(f\"True DAG adjacency matrix:\")\n",
    "print(problem.DAG)\n",
    "print(f\"\\nTrue B matrix:\")\n",
    "print(problem.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903209c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observational data shape: (5, 50)\n",
      "First 3 samples:\n",
      "[[ 1.61876236  0.24933486  0.07729819]\n",
      " [-0.57346892 -0.18289798 -0.46155853]\n",
      " [-0.49036561  0.48897721 -0.50712046]\n",
      " [ 0.59853702  0.26896469 -0.81159822]\n",
      " [ 0.10436958 -0.54133698 -0.0250681 ]]\n",
      "\n",
      "Intervention 1: a = [ 0.23712045  0.00590872  0.67162791  0.29897927 -0.63503253]\n",
      "Batch shape: (5, 20)\n",
      "\n",
      "Intervention 2: a = [ 0.10660719  0.50722503 -0.57473855 -0.2769506   0.56950122]\n",
      "Batch shape: (5, 20)\n",
      "\n",
      "Intervention 3: a = [-0.21976944 -0.06281145  0.41489099 -0.77676178 -0.41504543]\n",
      "Batch shape: (5, 20)\n",
      "\n",
      "Total batches: 4\n"
     ]
    }
   ],
   "source": [
    "# Generate observational data\n",
    "n_obs = 50\n",
    "X_obs = problem.sample(a=np.zeros((p, 1)), n=n_obs)\n",
    "\n",
    "print(f\"Observational data shape: {X_obs.shape}\")  # Should be (p, n_obs)\n",
    "print(f\"First 3 samples:\\n{X_obs[:, :3]}\")\n",
    "\n",
    "# Generate 3 intervention batches\n",
    "n_per_batch = 20\n",
    "batches = [X_obs]  # Start with observational\n",
    "interventions = [np.zeros((p, 1))]  # a=0 for observational\n",
    "\n",
    "for i in range(3):\n",
    "    # Random intervention\n",
    "    a = np.random.uniform(-1, 1, (p, 1))\n",
    "    a = a / np.linalg.norm(a)\n",
    "    \n",
    "    batch = problem.sample(a, n_per_batch)\n",
    "    batches.append(batch)\n",
    "    interventions.append(a)\n",
    "    \n",
    "    print(f\"\\nIntervention {i+1}: a = {a.flatten()}\")\n",
    "    print(f\"Batch shape: {batch.shape}\")\n",
    "\n",
    "print(f\"\\nTotal batches: {len(batches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eeae5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (110, 5)\n",
      "Imat shape: (110, 4)\n",
      "\n",
      "Intervention matrix (first 10 rows):\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Intervention matrix (check each condition):\n",
      "Condition 0: 50 samples\n",
      "Condition 1: 20 samples\n",
      "Condition 2: 20 samples\n",
      "Condition 3: 20 samples\n"
     ]
    }
   ],
   "source": [
    "def create_intervention_matrix(batches):\n",
    "    \"\"\"\n",
    "    Convert list of batches to format expected by BiDAG.\n",
    "    \n",
    "    Args:\n",
    "        batches: list of (p, n_i) arrays\n",
    "        \n",
    "    Returns:\n",
    "        data: (total_samples, p) array\n",
    "        Imat: (total_samples, n_conditions) binary array\n",
    "    \"\"\"\n",
    "    n_conditions = len(batches)\n",
    "    total_samples = sum(batch.shape[1] for batch in batches)\n",
    "    p = batches[0].shape[0]\n",
    "    \n",
    "    # Pre-allocate\n",
    "    data = np.zeros((total_samples, p))\n",
    "    Imat = np.zeros((total_samples, n_conditions))\n",
    "    \n",
    "    # Fill in data and intervention indicators\n",
    "    sample_idx = 0\n",
    "    for cond_idx, batch in enumerate(batches):\n",
    "        n_samples = batch.shape[1]\n",
    "        \n",
    "        # Transpose batch from (p, n) to (n, p) and insert\n",
    "        data[sample_idx:sample_idx + n_samples, :] = batch.T\n",
    "        \n",
    "        # Mark which condition these samples came from\n",
    "        Imat[sample_idx:sample_idx + n_samples, cond_idx] = 1\n",
    "        \n",
    "        sample_idx += n_samples\n",
    "    \n",
    "    return data, Imat\n",
    "\n",
    "\n",
    "# Test it\n",
    "data, Imat = create_intervention_matrix(batches)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")  # Should be (total_samples, p)\n",
    "print(f\"Imat shape: {Imat.shape}\")  # Should be (total_samples, 4)\n",
    "print(f\"\\nIntervention matrix (first 10 rows):\")\n",
    "print(Imat[:10, :])\n",
    "print(f\"\\nIntervention matrix (check each condition):\")\n",
    "for i in range(Imat.shape[1]):\n",
    "    print(f\"Condition {i}: {int(Imat[:, i].sum())} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49109249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ibge_score(data, Imat, dag_adjacency, am=0.1):\n",
    "    \"\"\"\n",
    "    Compute iBGe score for a given DAG.\n",
    "    \n",
    "    Args:\n",
    "        data: (n_samples, p) numpy array\n",
    "        Imat: (n_samples, n_conditions) binary numpy array  \n",
    "        dag_adjacency: (p, p) binary adjacency matrix\n",
    "        am: hyperparameter (default 0.1)\n",
    "    \n",
    "    Returns:\n",
    "        score: log iBGe score\n",
    "    \"\"\"\n",
    "    # Define R function to compute score (only once)\n",
    "    ro.r('''\n",
    "        compute_score_ibge <- function(data, Imat, dag, am) {\n",
    "            library(BiDAG)\n",
    "            \n",
    "            # Create scoring object with iBGe\n",
    "            scoreObj <- scoreparameters(\n",
    "                scoretype = \"usr\",\n",
    "                data = data,\n",
    "                usrpar = list(pctesttype = \"bge\", Imat = Imat, am = am)\n",
    "            )\n",
    "            \n",
    "            # Compute score for this DAG\n",
    "            score <- DAGscore(scoreObj, dag)\n",
    "            \n",
    "            return(score)\n",
    "        }\n",
    "    ''')\n",
    "    \n",
    "    # Convert numpy arrays to R objects within conversion context\n",
    "    with localconverter(ro.default_converter + numpy2ri.converter):\n",
    "        r_data = ro.conversion.py2rpy(data)\n",
    "        r_Imat = ro.conversion.py2rpy(Imat)\n",
    "        r_dag = ro.conversion.py2rpy(dag_adjacency.astype(int))\n",
    "    \n",
    "    # Call the R function\n",
    "    compute_fn = ro.r['compute_score_ibge']\n",
    "    score = compute_fn(r_data, r_Imat, r_dag, am)[0]\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a0618b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: In addition:   \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: Warning messages:\n",
      "  \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: 1:   \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: In (function (package, help, pos = 2, lib.loc = NULL, character.only = FALSE,  :  \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: \n",
      "   \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console:  libraries ‘/home/wvir/R/library’, ‘/usr/local/lib/R/site-library’, ‘/usr/lib/R/site-library’ contain no packages\n",
      "  \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: 2:   \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: In (function (package, help, pos = 2, lib.loc = NULL, character.only = FALSE,  :  \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: \n",
      "   \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console:  libraries ‘/usr/local/lib/R/site-library’, ‘/usr/lib/R/site-library’ contain no packages\n",
      "  \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: 3:   \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: In (function (package, help, pos = 2, lib.loc = NULL, character.only = FALSE,  :  \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console: \n",
      "   \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R callback write-console:  libraries ‘/usr/local/lib/R/site-library’, ‘/usr/lib/R/site-library’ contain no packages\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True DAG adjacency:\n",
      "[[0 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "\n",
      "Computing iBGe score...\n",
      "✓ iBGe score for TRUE DAG: -60.94\n"
     ]
    }
   ],
   "source": [
    "# Use the correctly formatted DAG\n",
    "true_dag = problem.DAG.to_amat()[0]  # Already extracted this above\n",
    "\n",
    "print(f\"True DAG adjacency:\\n{true_dag}\")\n",
    "print(f\"\\nComputing iBGe score...\")\n",
    "\n",
    "# Compute iBGe score\n",
    "score_true = compute_ibge_score(data, Imat, true_dag, am=0.1)\n",
    "\n",
    "print(f\"✓ iBGe score for TRUE DAG: {score_true:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "942d8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong DAG adjacency:\n",
      "[[0 0 0 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "\n",
      "Computing iBGe score...\n",
      "\n",
      "==================================================\n",
      "iBGe score for WRONG DAG: -101.36\n",
      "iBGe score for TRUE DAG:  -60.94\n",
      "==================================================\n",
      "Difference: 40.41\n",
      "\n",
      "✓ TRUE DAG has HIGHER score!\n"
     ]
    }
   ],
   "source": [
    "# Create a random wrong DAG\n",
    "np.random.seed(123)\n",
    "wrong_dag = np.random.binomial(1, 0.3, size=(p, p))\n",
    "# Make it acyclic by keeping only upper triangular\n",
    "wrong_dag = np.triu(wrong_dag, k=1)\n",
    "\n",
    "print(f\"Wrong DAG adjacency:\\n{wrong_dag}\")\n",
    "print(f\"\\nComputing iBGe score...\")\n",
    "\n",
    "score_wrong = compute_ibge_score(data, Imat, wrong_dag, am=0.1)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"iBGe score for WRONG DAG: {score_wrong:.2f}\")\n",
    "print(f\"iBGe score for TRUE DAG:  {score_true:.2f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Difference: {score_true - score_wrong:.2f}\")\n",
    "print(f\"\\n{'✓ TRUE DAG has HIGHER score!' if score_true > score_wrong else '✗ Something is wrong...'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "facfd09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned DAG object: [4][2][1][0][3|0,1]\n",
      "\n",
      "Learned DAG adjacency matrix:\n",
      "[[0 0 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "\n",
      "==================================================\n",
      "iBGe score for TRUE DAG:    -60.94\n",
      "iBGe score for LEARNED DAG: -73.42\n",
      "iBGe score for WRONG DAG:   -101.36\n",
      "==================================================\n",
      "\n",
      "Structural Hamming Distance (GES vs True): 2\n"
     ]
    }
   ],
   "source": [
    "from optint.utils import learn_dag\n",
    "\n",
    "# Learn DAG from the combined data using GES\n",
    "all_data = np.hstack(batches)  # Concatenate all batches\n",
    "learned_dag_ges_obj = learn_dag(all_data, p)\n",
    "\n",
    "print(f\"Learned DAG object: {learned_dag_ges_obj}\")\n",
    "\n",
    "# Extract adjacency matrix\n",
    "learned_dag_ges = learned_dag_ges_obj.to_amat()[0]\n",
    "print(f\"\\nLearned DAG adjacency matrix:\")\n",
    "print(learned_dag_ges)\n",
    "\n",
    "# Score the learned DAG using iBGe\n",
    "score_ges = compute_ibge_score(data, Imat, learned_dag_ges, am=0.1)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"iBGe score for TRUE DAG:    {score_true:.2f}\")\n",
    "print(f\"iBGe score for LEARNED DAG: {score_ges:.2f}\")\n",
    "print(f\"iBGe score for WRONG DAG:   {score_wrong:.2f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Check structural hamming distance\n",
    "from optint.utils import compute_shd\n",
    "shd = compute_shd(learned_dag_ges_obj, problem.DAG)  # Pass DAG objects, not arrays\n",
    "print(f\"\\nStructural Hamming Distance (GES vs True): {shd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e4adfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bayesian_active_ibge(problem, G_init, opts, K=1, am=0.1):\n",
    "    \"\"\"\n",
    "    Active learning with DAG bootstrap and iBGe scoring.\n",
    "    \n",
    "    Args:\n",
    "        problem: synthetic problem instance\n",
    "        G_init: initial DAG guess\n",
    "        opts: options (T, W, n, etc.)\n",
    "        K: number of bootstrap samples (K=1 for single DAG)\n",
    "        am: iBGe hyperparameter\n",
    "    \n",
    "    Returns:\n",
    "        A: list of interventions\n",
    "        Prob: list of posterior distributions\n",
    "        SHD: list of structural hamming distances\n",
    "    \"\"\"\n",
    "    from optint.model import linearSCM\n",
    "    from optint.acquisition import civ_acq\n",
    "    from optint.utils import learn_dag, compute_shd\n",
    "    \n",
    "    A = []\n",
    "    Prob = []\n",
    "    SHD = []\n",
    "    \n",
    "    all_batches = []\n",
    "    all_interventions = []\n",
    "    \n",
    "    # Start with initial DAG\n",
    "    current_dag = G_init.to_amat()[0]  # Convert to numpy array\n",
    "    model = linearSCM(G_init, {'pot_vec': 0, 'info_mat': 1})\n",
    "    \n",
    "    print(f\"Starting active learning with K={K} bootstrap samples\")\n",
    "    print(f\"Initial SHD: {compute_shd(G_init, problem.DAG)}\")\n",
    "    \n",
    "    # Warm-up with random interventions\n",
    "    for w in range(opts.W):\n",
    "        a = np.random.uniform(-1, 1, (problem.nnodes, 1))\n",
    "        a = a / np.linalg.norm(a)\n",
    "        \n",
    "        batch = problem.sample(a, opts.n)\n",
    "        model.update_posterior(a, batch)\n",
    "        \n",
    "        all_batches.append(batch)\n",
    "        all_interventions.append(a)\n",
    "        \n",
    "        A.append(a)\n",
    "        Prob.append(model.prob_padded())\n",
    "        SHD.append(compute_shd(G_init, problem.DAG))\n",
    "    \n",
    "    print(f\"Warm-up complete: {opts.W} random interventions\")\n",
    "    \n",
    "    # Active learning loop\n",
    "    for i in range(opts.T - opts.W):\n",
    "        # Get posterior for acquisition function\n",
    "        prob_pad = model.prob_padded()\n",
    "        mean = np.array(prob_pad['mean'])\n",
    "        var = np.array(prob_pad['var'])\n",
    "        \n",
    "        sigma_square = problem.sigma_square if opts.known_noise else np.zeros(problem.sigma_square.shape)\n",
    "        \n",
    "        # Compute acquisition function\n",
    "        acquisition = civ_acq(sigma_square, mean, var, problem.mu_target, opts.n, opts.measure)\n",
    "        \n",
    "        # Optimize acquisition (same as before)\n",
    "        try:\n",
    "            a_jitter = A[-1].reshape(-1) * np.random.uniform(0.8, 1.2, (problem.nnodes,))\n",
    "            x01 = np.maximum(np.minimum(a_jitter, 1), -1)\n",
    "            \n",
    "            x02 = problem.mu_target - np.matmul(mean, problem.mu_target)\n",
    "            x02 = x02.flatten()\n",
    "            x02 = x02 / np.linalg.norm(x02)\n",
    "            \n",
    "            a1 = acquisition.optimize(x0=x01)\n",
    "            a2 = acquisition.optimize(x0=x02)\n",
    "            \n",
    "            if a1 is not None and a2 is not None:\n",
    "                a1 = a1.reshape(-1, 1)\n",
    "                a2 = a2.reshape(-1, 1)\n",
    "                a = (a1, a2)[acquisition.evaluate(a1) > acquisition.evaluate(a2)]\n",
    "            elif a1 is not None:\n",
    "                a = a1.reshape(-1, 1)\n",
    "            elif a2 is not None:\n",
    "                a = a2.reshape(-1, 1)\n",
    "            else:\n",
    "                print(\"Both initializations failed...\")\n",
    "                a = np.random.uniform(-1, 1, problem.nnodes).reshape(-1, 1)\n",
    "                a = a / np.linalg.norm(a)\n",
    "        except:\n",
    "            x0 = np.random.uniform(-1, 1, problem.nnodes)\n",
    "            x0 = x0 / np.linalg.norm(x0)\n",
    "            a = acquisition.optimize(x0=x0)\n",
    "            if a is None:\n",
    "                a = np.random.uniform(-1, 1, problem.nnodes).reshape(-1, 1)\n",
    "                a = a / np.linalg.norm(a)\n",
    "            else:\n",
    "                a = a.reshape(-1, 1)\n",
    "        \n",
    "        # Sample from true system\n",
    "        batch = problem.sample(a, opts.n)\n",
    "        all_batches.append(batch)\n",
    "        all_interventions.append(a)\n",
    "        \n",
    "        # === DAG LEARNING WITH iBGe ===\n",
    "        # Format data for iBGe\n",
    "        data_for_scoring, Imat = create_intervention_matrix(all_batches)\n",
    "        \n",
    "        if K == 1:\n",
    "            # Single DAG: just learn with GES and score with iBGe\n",
    "            all_data = np.hstack(all_batches)\n",
    "            learned_dag_obj = learn_dag(all_data, problem.nnodes)\n",
    "            learned_dag = learned_dag_obj.to_amat()[0]\n",
    "            \n",
    "            score = compute_ibge_score(data_for_scoring, Imat, learned_dag, am=am)\n",
    "            \n",
    "            current_dag = learned_dag\n",
    "            current_dag_obj = learned_dag_obj\n",
    "            \n",
    "        else:\n",
    "            # DAG Bootstrap: sample K DAGs\n",
    "            dags = []\n",
    "            dag_objs = []\n",
    "            scores = []\n",
    "            \n",
    "            for k in range(K):\n",
    "                # Bootstrap sample the data\n",
    "                n_samples = sum(batch.shape[1] for batch in all_batches)\n",
    "                bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                \n",
    "                # Reconstruct batches from bootstrap indices\n",
    "                # (This is simplified - you might want to bootstrap per-batch)\n",
    "                all_data = np.hstack(all_batches)\n",
    "                bootstrap_data = all_data[:, bootstrap_indices]\n",
    "                \n",
    "                # Learn DAG on bootstrap sample\n",
    "                dag_obj_k = learn_dag(bootstrap_data, problem.nnodes)\n",
    "                dag_k = dag_obj_k.to_amat()[0]\n",
    "                \n",
    "                # Score on FULL dataset (not bootstrap)\n",
    "                score_k = compute_ibge_score(data_for_scoring, Imat, dag_k, am=am)\n",
    "                \n",
    "                dags.append(dag_k)\n",
    "                dag_objs.append(dag_obj_k)\n",
    "                scores.append(score_k)\n",
    "            \n",
    "            # Compute importance weights (unnormalized posterior)\n",
    "            scores = np.array(scores)\n",
    "            # Convert log scores to probabilities (numerically stable)\n",
    "            max_score = np.max(scores)\n",
    "            exp_scores = np.exp(scores - max_score)\n",
    "            weights = exp_scores / np.sum(exp_scores)\n",
    "            \n",
    "            # Use MAP DAG for now (highest scoring)\n",
    "            best_idx = np.argmax(scores)\n",
    "            current_dag = dags[best_idx]\n",
    "            current_dag_obj = dag_objs[best_idx]\n",
    "            \n",
    "            print(f\"Step {i+1}: K={K} DAGs, iBGe scores: [{scores.min():.1f}, {scores.max():.1f}], weights: {weights}\")\n",
    "        \n",
    "        # Update model with new DAG\n",
    "        model = linearSCM(current_dag_obj, {'pot_vec': 0, 'info_mat': 1})\n",
    "        \n",
    "        # Re-update posterior with all historical data\n",
    "        for batch_k, a_k in zip(all_batches, all_interventions):\n",
    "            model.update_posterior(a_k, batch_k)\n",
    "        \n",
    "        A.append(a)\n",
    "        Prob.append(model.prob_padded())\n",
    "        SHD.append(compute_shd(current_dag_obj, problem.DAG))\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Step {i+1}/{opts.T-opts.W}: SHD = {SHD[-1]}\")\n",
    "    \n",
    "    return A, Prob, SHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96aaa417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DAG SHD: 2\n",
      "\n",
      "============================================================\n",
      "TESTING WITH K=1 (single DAG)\n",
      "============================================================\n",
      "Starting active learning with K=1 bootstrap samples\n",
      "Initial SHD: 2\n",
      "Warm-up complete: 3 random interventions\n",
      "Step 10/17: SHD = 2\n",
      "\n",
      "Final SHD: 2\n",
      "SHD trajectory: [2, 8, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh problem for testing\n",
    "p = 10\n",
    "seed = 100\n",
    "\n",
    "problem_test = synthetic_instance(\n",
    "    nnodes=p,\n",
    "    DAG_type=\"random\",\n",
    "    sigma_square=0.4 * np.ones(p),\n",
    "    a_size=p,\n",
    "    a_target_nodes=None,\n",
    "    std=False,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Initial observational data\n",
    "X_obs = problem_test.sample(a=np.zeros((p, 1)), n=100)\n",
    "G_init = learn_dag(X_obs, p)\n",
    "\n",
    "print(f\"Initial DAG SHD: {compute_shd(G_init, problem_test.DAG)}\")\n",
    "\n",
    "# Set up options\n",
    "class Opts:\n",
    "    pass\n",
    "\n",
    "opts = Opts()\n",
    "opts.T = 20  # Total steps\n",
    "opts.W = 3   # Warm-up steps\n",
    "opts.n = 10  # Samples per intervention\n",
    "opts.known_noise = True\n",
    "opts.measure = 'unif'\n",
    "\n",
    "# Run with K=1\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING WITH K=1 (single DAG)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "A, Prob, SHD = test_bayesian_active_ibge(problem_test, G_init, opts, K=1)\n",
    "\n",
    "print(f\"\\nFinal SHD: {SHD[-1]}\")\n",
    "print(f\"SHD trajectory: {SHD[::5]}\")  # Every 5 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a47d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optint-unknown-dag",
   "language": "python",
   "name": "optint-unknown-dag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
